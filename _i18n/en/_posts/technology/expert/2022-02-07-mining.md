---
layout: post
type: article
title: "Mining"
description: "The Horizen Academy is a free educational platform on blockchain technology, cryptocurrency, and privacy. This chapter is is not available yet. We add content frequently, sign up for our newsletter for notifications when it's released."
permalink: /technology/expert/mining/
topic: technology
level: expert
chapter: "How Does a Blockchain Work?"
---

\url{https://medium.com/coinshares/an-honest-explanation-of-price-hashrate-bitcoin-mining-network-dynamics-f820d6218bdf} Part 1 of 2

\url{https://medium.com/@arjunblj/grin-and-the-mythical-fair-launch-395ca87a5e73}

selfish mining paper (in folder) \url{https://arxiv.org/pdf/1311.0243.pdf}
VB on selfish mining: \url{https://bitcoinmagazine.com/articles/selfish-mining-a-25-attack-against-the-bitcoin-network-1383578440}

\url{https://medium.com/@ifdefelse/the-cost-of-asic-design-a44f9a065b72}

SHA256(886)=000f21.....

Mining economics covered in detail here: \url{https://medium.com/@colin_/cryptocurrency-mining-swaps-a9abab47cacd}

### Intro

Mining is more than just performing Proof of Work. It has very interesting economic applications, from choosing which blockchain to mine, to what hardware to use and what strategies to apply. In this article we want to take a wholistic view at the mining industry. Mining has become a large business, and it would be misleading not to account for this in one way or another.

**TKKG**
This is less of a technical article, but one meant to provide an overview and a mental model to add further information to when it is needed. We will often use the term decentralizing and centralizing to express in which direction a given development shifts the overall level of coordinance. There is no absolute decentralization nor centralization and a system is moving on a scale between the two. When we talk about a decentralizing factor this does not mean it makes the system entirely decentralized, but it moves it on the centralization scale towards a more decentralized one.

### What a Miner Does

On a purely technical level it is straightforward to explain mining. It describes the activity of performing a computationally expensive task - the [Proof of Work (PoW)]({{ site.baseurl }}{% post_url /technology/expert/2022-02-05-2-proof-of-work %}). In the simplest case, this task is finding a *nonce* that when hashed together with the rest of the *block header* produces a block hash below a certain threshold which is called the *target*.

This type of PoW is called *Hashcash* Proof of Work and was introduced by Adam Back in 1997 as a measure against spam emails. The idea is simple: adding a small PoW to an email does not significantly impact an honest user sending only a few mails per day. Automated spam on the other hand would become computationally expensive because every single email would need to have a valid nonce attached.

The largest mining ecosystem has evolved around Bitcoin, which uses Hashcash style PoW using the SHA256 [hash function]({{ site.baseurl }}{% post_url /technology/expert/2022-02-03-hash-functions %}).

![Hash Cash Style PoW](/assets/post_files/technology/expert/2.6-mining/hash_cash_pow.jpg)

The graphic above shows the mining process in a Hashcash style PoW scheme. The block header is repeatedly hashed using a different variable for the nonce. The header is hashed twice using SHA256 and the resulting candidate block hash is compared to the current target. If it is greater than the target the nonce is discarded. Once a miner finds a nonce producing a block hash less than or equal to the target it is broadcast to the network, verified by all other nodes (including other mining nodes) and appended to every copy of the blockchain.

The *difficulty* is a different way to express the current target. The difficulty is a relative measure of the current target compared to the initial *maximum target* that is defined with the genesis block of a blockchain.

$$Difficulty = \frac{max. target}{target}$$

On the Bitcoin network, which has seen the biggest growth in hash power since its inception the current difficulty (at the time of writing) is about \\(1.3 \cdot 10^{13}\\), meaning it is 13 trillion times harder to find a block today, than it was when the protocol was launched.

### The Purpose of Miners

Now you might ask why it is necessary to make block production computationally hard if we want to continuously add blocks to the blockchain. The reason is that PoW is an efficient way to provide *Sybil resistance*. In a [Sybil Attack]({{ site.baseurl }}{% post_url /technology/expert/2022-06-04-ddos-sybil-eclipse %}) an attacker creates a large number of malicious node in an effort to achieve some goal. In the blockchain context, this goal can be including malicious transactions in a block or performing a double spend after a block reorganization.

Spinning up a node comes at a very low cost, so there is not much to prevent an attacker from creating a large number of them. A [consensus mechanism] represents an abstract form of voting on different versions of the same events. If a vote was tied to an IP address an attacker could manipulate the vote easily by creating Sybil nodes. By tying the voting power to external costs - electricity and hardware, interfering with the voting process on blocks becomes much harder.

Not only does PoW prevent Byzantine actors from obstructing block production, but it also protects the history recorded on the blockchain from being tampered with. All nodes following the [protocol of a blockchain]({{ site.baseurl }}{% post_url /technology/expert/2022-01-03-a-protocol-to-transfer-value %}) will reject a version of the blockchain that does not have a chain of references form the current block all the way back to the genesis block.

A node with access to the most recent protocol can verify the entire blockchain with all its transactions without having to rely on any external input. It will start with the first block, reconstruct the Merkle tree of transactions to verify the Merkle root included in the block header, verify if the block hash meets the target given the nonce used to produce it and finally moves on to the next block. It will even know independently when the difficulty requirements changed based on the blocks timestamps and evaluate blocks after a difficulty adjustment happened based on that requirement.

What this means is that every computational step in a PoW blockchain adds additional security to its transaction history and strenghtens the network.

+++ block reward?

For their contribution miners are allowed to include the *coinbase transaction* sending an address of their choice the current *block subsidy* and the aggregate *transaction fees* included in the block - the *block reward*. After the *block reward maturation period* ends this output becomes spendable. **TKKG: maturation period 99, 396??**

### Mining Algorithms

In the article on PoW we went over the criteria defining a "good" PoW and at this point we would like to recap some of those properties.

- *Asymmetry*: The Proof of Work needs to be hard to produce, but easy to verify.
- *Optimization Free*: An optimization free algorithms means, that you cannot improve the efficiency of performing that task through software or algorithmic  improvements.
- *Amortization free*: This means economies of scale that would render all mining pools besides the largest one irrelevant should not be possible based on the algorithm. Other aspects of mining will always enable Economies of scale to play out.
- *Independently Tunable Parameters*: Certain parameters of the problem to solve by the miners should be easy to adapt.

First, the PoW needs to be pseudorandom so there is no shortcut to taking a brute-force approach of solving the task at hand. This also ensures that the block production time follows a statistical distribution with a constant average, *ceteris paribus*. 
Next, the PoW algorithm needs to have independently adjustable parameters to account for changes in the overall hash power. To keep the block production rate constant as more hash power is added to the network the difficulty can increase. On the Horizen network this *difficulty adjustment* is performed after every block. **TKKG**

$$next difficulty = last difficulty \cdot \sqrt{\frac{150 sec}{last solve time}}$$

Lastly, the algorithm needs to be *deterministic* so the verification of the Proof is guranteed.

**MOVE TO POW**

Let us take a look at the parameters of a PoW scheme, that allow us to tune it. The most intuitive parameter to understand is the target. Because cryptographic hash functions map their inputs evenly distributed across the output range, lowering the threshold a hash has to be smaller than makes the task of finding an according input more difficult. The more hash power is on the network, the lower the average time until such an input is found. This means shorter block times. Lowering the target naturally increases the block time.

There are much more complex PoW algorithms though. One of the more widely used ones is [*Equihash*](https://www.cryptolux.org/images/b/b9/Equihash.pdf), which is based on the *Generalized Birthday Problem* and used by the Horizen blockchain.

++++ birthday problem

The [*birthday problem*](\url{https://en.wikipedia.org/wiki/Birthday_problem}) describes an interesting property regarding a group of people and their birthdays. In a group of 367 people, the probability of two of those people sharing a birthday is 1, as their is one more person than days in a leap year. However, with just 23 people in the group the chance of two of them sharing a birthday is already at 50\%, while it reaches 99.9\% with just 70 people.

If you find this interesting or have a hard time believing it, you might also want to look at the closely related [*Pigeonhole Principle*](https://en.wikipedia.org/wiki/Pigeonhole_principle) the [*Hilbert's paradox of the Grand Hotel*](https://medium.com/i-math/hilberts-infinite-hotel-paradox-ca388533f05l) or [*The Traveling Salesman Problem*](https://en.wikipedia.org/wiki/Travelling_salesman_problem). Each can turn into a rabbit hole of its own.

++++ birthday problem generalized

The [*generalized birthday problem*](https://en.wikipedia.org/wiki/Birthday_problem#The_generalized_birthday_problem) refers to the difficulty of calculating those probabilities in a more general context. We define a time period of *d* days, and ask how many people *n* do we need to have a 50% likelihood of a birthday coincidence. 

++++ birthday problem gen md

Next, we can increase the number of dimensions for the problem. Instead of looking at a group of *n* people, we could look at *k* groups of \\(n_k\\) group members. With a given set of values for *d*, *k* and \\(n_0, n_1,... n_k\\), what is the likelihood of a man and a woman sharing a birthday?

Starting from the generalized birthday problem, David Wagner devised an algorithm - the *Wagner's Algorithm* - to compute those probabilities for even more complex setting with *k* dimensions (groups of people) and *n*-bit settings (days). The algorithm [was found to be](https://pdfs.semanticscholar.org/06f4/507d9f584b544f96364cae2ad41e78e4035b.pdf) hard on time and space.

The time and space complexity trade-off can be adjusted by the parameter *k*. This means, very loosely speaking, that if one uses the generalized birthday problem for a PoW algorithm, one can make it more CPU or memory intensive depending on the number of groups *k* a birthday collision is calculated for.

Horizen has implemented Equihash with *n*=200 and *k*=9.**???**

Some other examples of computationally hard problems used to create a Proof of Work include finding prime numbers and solving the traveling salesman problem. In Primecoin the PoW task is to find prime number chains e.g. *Cunningham chains* and *Bi-twin chains* where primes roughly double or follow a specific sequence. The [Cuckoo Cycle*] algorithm by Tromp et al. is based on graph theory. It comes down to the Traveling Salesman Problem and is implemented in Grin and Ã¦ternity.

Another interesting concept was presented with ProgPoW. Its algorithm is designed to match already available hardware to reverse the scenario that has played out in the past where the companies with the largest budget were able to bring specialized mining hardware for a given algorithm to market first.

**TKKG: Include Link??? Check paragraph above for accuracy** https://medium.com/@ifdefelse/understanding-progpow-performance-and-tuning-d72713898db3

### Mining Hardware

In our article on PoW we introduced two metrics to assess computational hardness. *Space complexity* refers to the memory intensiveness of the computation, while *time complexity* describes the reliance on the processor.

When a PoW algorithm involves creating large data sets in some intermediary steps this will increase the overall space complexity. Other PoW algorithms barely require any memory as they involve repeated hashing of small amounts of data in the kilo byte range. By concatenating many computations on this small data set the time complexity of the algorithm will increase with the space complexity remaining constant.

When it comes to hardware there is a tradeoff between versatility and performance. A highly specialized chip architecture will allow a very good performance on one task, at the cost of being less versatile and vice versa.

++++ cpu asics graphic

The central processing unit (CPU) of a computer offers the greatest flexibility with regards to the types of computations it can perform, at the cost of being not very performant. The graphical processing unit (GPU) is focused on tasks around image processing as the name suggests. It also includes more memory because computations on graphics tend to have a higher space complexity. Field-programmable gate arrays (FPGAs) are meant to be configured for a specific task after manufacturing which allows a specialization of the integrated circuit depending on its application. After configuration they represent another step towards specialization at the expense of versatility. As the name suggests, the Application Specific Integrated Circuit (ASIC) sits on the end of the spectrum on specialization and allows for the very efficient computation of the tasks it was designed for.

In the early days of cryptocurrency mining people where doing so using the hardware they had at hand - desktop computers and their CPUs. In October of 2010 the GPU mining became feasible when the first software utilizing the graphical processing unit for mining was released to the public. People with a gaming setup were now at a serious advantage. Mining started to become more lucrative and hence more competitive so people started configuring FPGAs to suit the needs of the mining algorithm. The last major evolutionary step in mining happened in 2013 when the first ASIC designed for cryptocurrency mining was released.

A number of different "ASIC resistant" mining algos were proposed but ASIC resistance algorithms are a myth to some degree. When a chip manufacturer designs an integrated circuit with a specific mining algorithm in mind it will always perform better than a more versatile GPU.
More realistically, the assumption was that the cost to design and produce ASICs for certain algorithms was just not worth the marginal improvement in efficiency. While this assumption was certainly justified at the time many of the mining algorithms emerged it didn't account for the rising valuations of cryptocurrencies that we saw in 2017. This caused a range of ASICs being developed for different mining algos, including some that were thought of as ASIC resistant.

ASIC resistance was mostly achieved by increasing the space complexity of the algorithm and thereby increasing the memory demand. High-bandwidth memory is a very expensive component in chip manufacturing. A general increase in demand for high-speed memory technology such as [GDDR6](https://en.wikipedia.org/wiki/GDDR6_SDRAM) and [HBM2](https://en.wikipedia.org/wiki/High_Bandwidth_Memory) made producing relatively cheap ASICs for memory intensive mining algorithms feasible.

The demand for ASIC resistant algorithms came largely from an interest in a decentralized mining economy which results in a more dezentralized distribution of coins. GPUs are more widely spread and accessible than the highly specialized ASICs. There are more people with access to GPUs **TKKG**

The demand for GPU mining was met with some projects changing their mining algorithm, rendering the highly specialized ASICs useless. But changing algorithms mostly benefits large hardware manufacturers as they have the resources to develop ASICs for the new algorith first. By keeping an algorithm, more and smaller companies have the chance to design and produce mining hardware, which decentralizes the business again.

Today, we are past the peak of mining centralization. For a period of time around 2017 and 2018 Bitmain used to be the dominant manufacturer for mining specifc hardware. Since than, their market share has constantly decreased and new players have entered the business.

If you are interested in a detailed look at the considerations around and cost of ASIC design [this article](https://medium.com/@ifdefelse/the-cost-of-asic-design-a44f9a065b72) is a very good starting point.

### How Mining has Professionalized

Besides using sophisticated and specialized hardware mining has professionalized in other ways. Economic principles known for decades and centuries increasingly started to apply to this field.

First, location factors are very important today, as the industry is highly competitive and electricity prices are an important factor determening if a mining operation is profitable or not.

Next, miners started to organize into mining pools. They prefer stable cash flows and have come up with mining pools to share the work and socialize the winnings. Mining pools have a centralizing effect and grant a lot of power to the pool operators. Mining pool software could be configured to exclude transactions involving certain addresses - something that cryptocurrencies try to avoid at great cost. [BetterHash](https://github.com/TheBlueMatt/bips/blob/betterhash/bip-XXXX.mediawiki) addresses this issue by allowing individual miners to create their own block templates.

Economies of scale are also starting to play out, which is not a bad thing *per se*. A mining operation will always be more profitable when it gets bulk discounts on hardware or electricity and the price per square meter tends to go down when you rent or buy larger warehouses. But large-scale mining operations operate in a highly competitive environment that leaves minimal margins when selling the mined coins. To cover operational costs there is constant selling pressure for these actors, which effectively makes it seem like coins are issued to the secondary market directly.

### Mining Economics

When you look at the economics from a miners perspective you first need to think of the cost of the operation, than consider the potential revenues and lastly derive an estimate of your profits.

The costs can be divided into two categories: *capital expenditures* (CAPEX) and *operational expenditures* (OPEX). CAPEX are the cost of aquiring non-monetary capital like mining gear, racks, and property (if it is bought).
OPEX are ongoing expenditures like wages, electricity cost for the rigs as well as cooling and property (if rented). Setting up a mining operation will usually come with both capital and operational expenditures but depending on the circumstances can be set up to tend towards being CAPEX or OPEX heavy. [This article](https://medium.com/@colin_/cryptocurrency-mining-swaps-a9abab47cacd) goes through the economics of setting up an operation that aims to mine one block a day on the Bitcoin network. Although the numbers are somewhat outdated, the considerations behing calculating costs and profits are very clearly explained.

The mining industry and the price of a given coin are arguably related to some extend. The *hashrate* will follow the price, becauese an increase in price will increase the profit margin of existing miners and lead to them taking more hardware live. Additionally, new entrants will join the network and try to get a share of the business. This increase in competition and the accommponied decrease in margins leads to the mining cost tending towards the price of a coin, minus a small margin.

The trailing effect of the hashrate following the price comes with a delay and the delay when prices go up is greater than in a situation where prices decline. Ordering new mining gear and getting this equipment live takes longer than removing hashrate from the network when prices decline.

Without the difficulty adjustment in place, a given amount of hashrate would always produce a fixed amount of coins. This would still allow for a fixed supply, but not a fixed issuance schedule. This is an interesting differentiating factor when comparing cryptocurrencies with gold. With gold, production will increase as prices rise. Less profitable inactive mining sites are reactivated as the expected *return on investment* (ROI) becomes positive and active mining facilities increase the flow rate. This also works the other way around when price declines and mining facilities are closing. This dampens the volatility because the supply can increase or decrease in tandem with price. With cryptocurrencies production is constant and can not display this effect.

An economic term often used in the context of commodities like gold and other metals is the *stock to flow ratio*. It describes the amount of a commodity in circulation relative to the production rate. Bitcoin was the first asset to ever allow not just for a predictable supply and issuance schedule, but also a predictable stock to flow ratio.

### Mining Strategies

intransparent business because highly competitive.

"We assume that miners are rational; that is, they
try to maximize their revenue, and may deviate from the protocol to do so." - selfish paper

There are a few strategies that evolved which are used in order to gain a competitive advantage. Block wihtholding/selfish mining is such a strategy.

https://bitcoinmagazine.com/articles/selfish-mining-a-25-attack-against-the-bitcoin-network-1383578440


+++++ graphic selfish mining.

usually, every node only mines and propagates the first of of two competing blocks that it sees

Why it doensn't work: higher level economic concerns: miners with significant share have a lot of tied up capital in their business, even then they operate opex heavy. Mining equiptment has to be bought. By mounting such an attack they would undermine the credibility of their source of income.. not smart.

Also to make this really feasible one would have to involve other miners and pools. High risk.

theorized about as early as 2010, didn't play out. but block withholding still done. especially when block is found very early.

some examples from Hash war ABC vs. SV around difficulty adjustment.

### Energy Consumption

not waste if paying for it and using it to achieve a subjectively valuable objective -> coins.

potential to even the energy price hyperplane

+++ hyperplane graphic

stranded energy, aluminium in island

renewable sources used more efficiently

peaks: day night, moderate temp vs hot (AC) and cold (heating) Summer in one hemisphere winter in the other (electricity doesn't travel well over very long distances)

pow is efficient


\subsubsection*{FR}

Sia Post for sure
pow is efficient
